# -*- coding: utf-8 -*-
import os, re, glob, time, sqlite3, gzip
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# ========== æ ¸å¿ƒè·¯å¾‘è¨­å®š ==========
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
AUDIT_DB_PATH = os.path.join(BASE_DIR, "data_warehouse_audit.db")
OUTPUT_BASE = os.path.join(BASE_DIR, "data", "processed_wmy")

# ========== å…§éƒ¨å·¥å…·å‡½å¼ ==========

def _parse_id_name(stem):
    """è§£æžæª”åï¼Œæå–ä»£è™Ÿ"""
    return stem.split('_')[0], stem

def _canonical_id(raw_id):
    """æ¨™æº–åŒ–ä»£è™Ÿæ ¼å¼"""
    return str(raw_id).strip().upper()

def _load_day_clean_full(path):
    """è¼‰å…¥ CSV ä¸¦é€²è¡ŒåŸºç¤Žæ¸…æ´—èˆ‡æ ¼å¼åŒ–"""
    df = pd.read_csv(path)
    # çµ±ä¸€æ¬„ä½åç¨±ç‚ºä¸­æ–‡ä»¥åˆ©å¾ŒçºŒé‚è¼¯ä¸€è‡´
    rename_map = {
        'date': 'æ—¥æœŸ', 'open': 'é–‹ç›¤', 'high': 'æœ€é«˜', 
        'low': 'æœ€ä½Ž', 'close': 'æ”¶ç›¤', 'volume': 'æˆäº¤é‡'
    }
    df = df.rename(columns=rename_map)
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    # ç§»é™¤ä»»ä½•ç©ºå€¼
    df = df.dropna(subset=['é–‹ç›¤', 'æœ€é«˜', 'æœ€ä½Ž', 'æ”¶ç›¤'])
    return df

def _resample_ohlc_with_flags(df, period):
    """
    åŸ·è¡Œ OHLC æŽ¡æ¨£è½‰æ›
    W=é€±, M=æœˆ, Y=å¹´
    """
    resampler = df.set_index('æ—¥æœŸ').resample(period)
    df_res = resampler.agg({
        'é–‹ç›¤': 'first',
        'æœ€é«˜': 'max',
        'æœ€ä½Ž': 'min',
        'æ”¶ç›¤': 'last',
        'æˆäº¤é‡': 'sum'
    }).dropna()
    return df_res.reset_index()

def _add_period_returns(df, period):
    """è¨ˆç®—é€±æœŸæ¼²è·Œå¹…"""
    if not df.empty:
        df['æ¼²è·Œå¹…'] = df['æ”¶ç›¤'].pct_change().round(4)
    return df

# ========== å¯©è¨ˆèˆ‡è™•ç†æ ¸å¿ƒ ==========

def record_conversion_audit(market_key, total, success, skip_records):
    """å°‡è½‰æ›çµæžœå¯«å…¥å¯©è¨ˆè³‡æ–™åº« (ä¿®æ­£ç‚º UTC+8)"""
    conn = sqlite3.connect(AUDIT_DB_PATH)
    try:
        conn.execute('''CREATE TABLE IF NOT EXISTS wmy_conversion_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            execution_time TEXT,
            market_id TEXT,
            total_files INTEGER,
            success_count INTEGER,
            skip_count INTEGER,
            success_rate REAL
        )''')
        # ä¿®æ­£æ™‚å€
        now = (datetime.utcnow() + timedelta(hours=8)).strftime("%Y-%m-%d %H:%M:%S")
        skip = len(skip_records)
        rate = round((success / total * 100), 2) if total > 0 else 0
        conn.execute('INSERT INTO wmy_conversion_audit (execution_time, market_id, total_files, success_count, skip_count, success_rate) VALUES (?,?,?,?,?,?)',
                     (now, market_key, total, success, skip, rate))
        conn.commit()
    finally:
        conn.close()

def _process_one_file(path: str):
    """æ•´åˆè³‡æ–™ç•°å¸¸åˆ†æžçš„æ ¸å¿ƒé‚è¼¯"""
    try:
        stem = Path(path).stem
        raw_id, _ = _parse_id_name(stem)
        cid = _canonical_id(raw_id)

        # 1. è¼‰å…¥èˆ‡åŸºç¤Žæ¸…æ´—
        df_day = _load_day_clean_full(path)
        if df_day.empty:
            return "SKIP", {'StockID': cid, 'reason': 'empty_file'}, "ç©ºæª”æ¡ˆ"
        
        # 2. ç•°å¸¸æª¢æŸ¥ï¼šåƒ¹æ ¼å¿…é ˆ > 0
        if (df_day['æ”¶ç›¤'] <= 0).any():
            return "SKIP", {'StockID': cid, 'reason': 'invalid_price'}, "åƒ¹æ ¼å«è² å€¼æˆ–é›¶"

        # 3. æ–·å±¤åµæ¸¬ï¼šæª¢æŸ¥ 2024 å¹´å¾Œæ˜¯å¦æœ‰è¶…éŽ 14 å¤©çš„è³‡æ–™æ–·å±¤
        # æ³¨æ„ï¼šå°æ–¼æ—¥è‚¡/ç¾Žè‚¡ï¼Œåœ‹å®šå‡æ—¥å¯èƒ½å°Žè‡´è¶…éŽ 14 å¤©çš„æ–·å±¤ï¼Œæ­¤è™•é–€æª»å¯è¦–æƒ…æ³èª¿æ•´
        df_check = df_day[df_day['æ—¥æœŸ'] >= '2024-01-01'].sort_values('æ—¥æœŸ')
        if not df_check.empty and len(df_check) > 1:
            gaps = df_check['æ—¥æœŸ'].diff().dt.days
            if gaps.max() > 14:
                return "SKIP", {'StockID': cid, 'reason': f'gap_{int(gaps.max())}d'}, "å­˜åœ¨é•·æ—¥æœŸæ–·å±¤"

        # 4. åŸ·è¡Œè½‰æ›
        dfw = _resample_ohlc_with_flags(df_day, 'W-FRI') # é€±äº”ç‚ºçµæŸ
        dfw = _add_period_returns(dfw, 'W'); dfw['StockID'] = cid
        
        dfm = _resample_ohlc_with_flags(df_day, 'M')
        dfm = _add_period_returns(dfm, 'M'); dfm['StockID'] = cid
        
        dfy = _resample_ohlc_with_flags(df_day, 'Y')
        dfy = _add_period_returns(dfy, 'Y'); dfy['StockID'] = cid

        # 5. OHLC é‚è¼¯é©—è­‰
        for _df in [dfw, dfm, dfy]:
            if ((_df['æ”¶ç›¤'] > _df['æœ€é«˜']) | (_df['æ”¶ç›¤'] < _df['æœ€ä½Ž'])).any():
                return "SKIP", {'StockID': cid, 'reason': 'ohlc_logic_error'}, "OHLC é‚è¼¯éŒ¯èª¤"

        return True, (dfw, dfm, dfy), None
    except Exception as e:
        return False, None, str(e)

# ========== ä¸»å…¥å£å‡½å¼ ==========

def main(market_id, input_dir):
    """
    ä¸»é€²å…¥é»žï¼šå°‡æŒ‡å®šç›®éŒ„çš„ CSV è½‰ç‚º WMY Parquet
    """
    print(f"ðŸ› ï¸ é–‹å§‹åŸ·è¡Œ {market_id} é€±æœŸè½‰æ›ä»»å‹™...")
    csv_files = glob.glob(os.path.join(input_dir, "*.csv"))
    total = len(csv_files)
    
    if total == 0:
        return {"total": 0, "success": 0, "fail": 0, "fail_list": []}

    success_count = 0
    fail_list = []
    
    all_w, all_m, all_y = [], [], []

    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        futures = {executor.submit(_process_one_file, f): f for f in csv_files}
        
        for future in as_completed(futures):
            res_type, data, reason = future.result()
            if res_type is True:
                success_count += 1
                w, m, y = data
                all_w.append(w); all_m.append(m); all_y.append(y)
            elif res_type == "SKIP":
                fail_list.append(f"{data['StockID']}({data['reason']})")
            else:
                fail_list.append(f"Error_{reason}")

    # å„²å­˜çµæžœç‚º Parquet (é«˜æ•ˆèƒ½å£“ç¸®æ ¼å¼)
    if all_w:
        market_out = os.path.join(OUTPUT_BASE, market_id)
        os.makedirs(market_out, exist_ok=True)
        
        pd.concat(all_w).to_parquet(os.path.join(market_out, "weekly.parquet"), index=False)
        pd.concat(all_m).to_parquet(os.path.join(market_out, "monthly.parquet"), index=False)
        pd.concat(all_y).to_parquet(os.path.join(market_out, "yearly.parquet"), index=False)

    # ç´€éŒ„å¯©è¨ˆ
    record_conversion_audit(market_id, total, success_count, fail_list)

    print(f"âœ… {market_id} è½‰æ›å®Œæˆï¼šæˆåŠŸ {success_count} / ç¸½é‡ {total}")
    
    return {
        "total": total,
        "success": success_count,
        "fail": len(fail_list),
        "fail_list": fail_list[:10] # åªå›žå‚³å‰ 10 ç­†çµ¦ Email å ±è¡¨
    }

if __name__ == "__main__":
    # æ¸¬è©¦ç”¨
    # main("tw-share", "./data/tw-share/dayK")
    pass
