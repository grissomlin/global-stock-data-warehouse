# -*- coding: utf-8 -*-
import os, io, time, random, sqlite3, requests
import pandas as pd
import yfinance as yf
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ========== 1. ç’°å¢ƒåˆ¤æ–·èˆ‡åƒæ•¸è¨­å®š ==========
MARKET_CODE = "us-share"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "us_stock_warehouse.db")
IS_GITHUB_ACTIONS = os.getenv('GITHUB_ACTIONS') == 'true'

# âœ… é€Ÿåº¦å„ªåŒ–è¨­å®š
BATCH_SIZE = 50        # æ¯æ‰¹æ¬¡è™•ç† 50 æª”è‚¡ç¥¨ (å¹³è¡¡é€Ÿåº¦èˆ‡è¢«å°é¢¨éšª)
MAX_WORKERS = 4 if IS_GITHUB_ACTIONS else 10 
# æ‰¹æ¬¡é–“çš„ç­‰å¾…æ™‚é–“
BATCH_DELAY = (3.0, 7.0) if IS_GITHUB_ACTIONS else (0.5, 1.0)

def log(msg: str):
    print(f"{pd.Timestamp.now():%H:%M:%S}: {msg}")

# ========== 2. è³‡æ–™åº«åˆå§‹åŒ– ==========

def init_db():
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute('''CREATE TABLE IF NOT EXISTS stock_prices (
                            date TEXT, symbol TEXT, open REAL, high REAL, 
                            low REAL, close REAL, volume INTEGER,
                            PRIMARY KEY (date, symbol))''')
        conn.execute('''CREATE TABLE IF NOT EXISTS stock_info (
                            symbol TEXT PRIMARY KEY, name TEXT, sector TEXT, market TEXT, updated_at TEXT)''')
        
        cursor = conn.execute("PRAGMA table_info(stock_info)")
        columns = [column[1] for column in cursor.fetchall()]
        if 'market' not in columns:
            conn.execute("ALTER TABLE stock_info ADD COLUMN market TEXT")
            conn.commit()
    finally:
        conn.close()

# ========== 3. ç²å–å¸¶æœ‰ç”¢æ¥­åˆ¥çš„ç¾Žè‚¡åå–® (åå–®å„ªåŒ–) ==========

def get_us_stock_list_with_sectors():
    """ç²å–ç¾Žè‚¡åå–®ä¸¦ç›´æŽ¥æ˜ å°„ç”¢æ¥­åˆ¥"""
    log("ðŸ“¡ æ­£åœ¨ç²å–ç¾Žè‚¡å®˜æ–¹æ¸…å–®èˆ‡ç”¢æ¥­å°ç…§è¡¨...")
    
    # ä¾†æºï¼šé«˜å“è³ªçš„é–‹æºç¾Žè‚¡å­—å…¸åº«
    ref_url = "https://raw.githubusercontent.com/rreichel3/US-Stock-Symbols/main/all/all_tickers.csv"
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(ref_url, headers=headers, timeout=20)
        df_ref = pd.read_csv(io.StringIO(r.text))
        
        # æº–å‚™å¯«å…¥ stock_info
        conn = sqlite3.connect(DB_PATH)
        items = []
        
        # éŽæ¿¾å¸¸è¦‹éžæ­£è‚¡é—œéµå­—
        exclude_kw = r"Warrant|Right|Preferred|Wrt|Unit"
        df_clean = df_ref[~df_ref['Name'].str.contains(exclude_kw, na=False, case=False)]
        
        for _, row in df_clean.iterrows():
            symbol = str(row['Ticker']).strip().upper()
            name = str(row['Name']).strip()
            sector = str(row.get('Sector', 'Unknown'))
            market = str(row.get('Exchange', 'Unknown'))
            
            if sector == 'nan': sector = 'Unknown'
            
            conn.execute("""
                INSERT OR REPLACE INTO stock_info (symbol, name, sector, market, updated_at) 
                VALUES (?, ?, ?, ?, ?)
            """, (symbol, name, sector, market, datetime.now().strftime("%Y-%m-%d")))
            items.append((symbol, name))
            
        conn.commit()
        conn.close()
        log(f"âœ… ç¾Žè‚¡æ¸…å–®åŒæ­¥æˆåŠŸ: {len(items)} æª” (å·²å¸¶å…¥ç”¢æ¥­è³‡è¨Š)")
        return items
    except Exception as e:
        log(f"âš ï¸ ç”¢æ¥­åå–®ç²å–å¤±æ•—: {e}ï¼Œä½¿ç”¨å‚™æ´æ©Ÿåˆ¶")
        return [("AAPL", "Apple"), ("TSLA", "Tesla")]

# ========== 4. æ‰¹é‡ä¸‹è¼‰ä¸‹è¼‰é‚è¼¯ (é€Ÿåº¦æå‡ 20 å€çš„é—œéµ) ==========

def download_batch(symbols_batch, mode):
    """æ‰¹é‡ä¸‹è¼‰ 50 æª”è‚¡ç¥¨æ•¸æ“š"""
    start_date = "2020-01-01" if mode == 'hot' else "2010-01-01"
    
    try:
        # ðŸ’¡ ä½¿ç”¨ yf.download é€²è¡Œæ‰¹é‡è«‹æ±‚
        data = yf.download(
            tickers=symbols_batch,
            start=start_date,
            group_by='ticker',
            auto_adjust=True,
            threads=False, # å…§éƒ¨ä¸é–‹åŸ·è¡Œç·’ï¼Œç”±æˆ‘å€‘å¤–éƒ¨æŽ§åˆ¶
            progress=False,
            timeout=30
        )
        
        if data.empty: return 0
        
        conn = sqlite3.connect(DB_PATH, timeout=60)
        success_in_batch = 0
        
        # è™•ç†ä¸‹è¼‰å›žä¾†çš„æ•¸æ“š
        for symbol in symbols_batch:
            try:
                # å–å¾—è©²æª”è‚¡ç¥¨çš„ DF
                if len(symbols_batch) > 1:
                    df = data[symbol].copy()
                else:
                    df = data.copy()
                
                df.dropna(how='all', inplace=True)
                if df.empty: continue
                
                df.reset_index(inplace=True)
                df.columns = [c.lower() for c in df.columns]
                
                # æ¨™æº–åŒ–æ—¥æœŸ
                df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None).dt.strftime('%Y-%m-%d')
                df['symbol'] = symbol
                
                # å¯«å…¥è³‡æ–™åº«
                final_df = df[['date', 'symbol', 'open', 'high', 'low', 'close', 'volume']]
                final_df.to_sql('stock_prices', conn, if_exists='append', index=False,
                                method=lambda t, c, k, d: c.executemany(
                                    f"INSERT OR REPLACE INTO {t.name} ({', '.join(k)}) VALUES ({', '.join(['?']*len(k))})", d))
                success_in_batch += 1
            except:
                continue
                
        conn.close()
        return success_in_batch
    except Exception as e:
        log(f"âš ï¸ æ‰¹æ¬¡ä¸‹è¼‰ç•°å¸¸: {e}")
        return 0

# ========== 5. ä¸»æµç¨‹ ==========

def run_sync(mode='hot'):
    start_time = time.time()
    init_db()
    
    items = get_us_stock_list_with_sectors()
    symbols = [it[0] for it in items]
    
    # å°‡æ¸…å–®åˆ†æˆ BATCH_SIZE ä¸€çµ„
    batches = [symbols[i:i + BATCH_SIZE] for i in range(0, len(symbols), BATCH_SIZE)]
    log(f"ðŸš€ é–‹å§‹ç¾Žè‚¡æ‰¹é‡åŒæ­¥ ({mode.upper()}) | ç¸½å…± {len(batches)} å€‹æ‰¹æ¬¡")

    total_success = 0
    
    # ä½µç™¼åŸ·è¡Œæ‰¹æ¬¡
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_batch = {executor.submit(download_batch, b, mode): b for b in batches}
        
        pbar = tqdm(total=len(symbols), desc="USåŒæ­¥ä¸­")
        for f in as_completed(future_to_batch):
            # ðŸ’¡ æ¯å€‹æ‰¹æ¬¡å®Œæˆå¾Œå¢žåŠ éš¨æ©Ÿç­‰å¾…ï¼Œé˜²æ­¢ IP è¢«å°
            time.sleep(random.uniform(*BATCH_DELAY))
            
            res = f.result()
            total_success += res
            pbar.update(BATCH_SIZE)
        pbar.close()

    # å„ªåŒ–
    log("ðŸ§¹ åŸ·è¡Œè³‡æ–™åº«å„ªåŒ–...")
    conn = sqlite3.connect(DB_PATH)
    conn.execute("VACUUM")
    conn.close()

    duration = (time.time() - start_time) / 60
    log(f"ðŸ“Š åŒæ­¥å®Œæˆï¼æˆåŠŸæ¨™çš„: {total_success} | è²»æ™‚: {duration:.1f} åˆ†é˜")
    
    return {
        "success": total_success,
        "total": len(symbols),
        "has_changed": total_success > 0
    }

if __name__ == "__main__":
    run_sync(mode='hot')
