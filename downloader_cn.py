# -*- coding: utf-8 -*-
import os, sys, time, random, json, subprocess, sqlite3
import pandas as pd
import yfinance as yf
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ========== è·¯å¾‘èˆ‡åƒæ•¸è¨­å®š ==========
MAX_WORKERS = 4 
DB_NAME = "cn_stock_warehouse.db"
# é‡å° Colab ç’°å¢ƒè¨­å®šå¿«å–è·¯å¾‘ï¼Œè‹¥åœ¨å…¶ä»–ç’°å¢ƒåŸ·è¡Œæœƒè‡ªå‹•åˆ‡æ›è‡³ç•¶å‰ç›®éŒ„
CACHE_DIR = "/content/drive/MyDrive/å„åœ‹è‚¡ç¥¨æª”æ¡ˆ/logs"
CACHE_FILE = os.path.join(CACHE_DIR, "cn_symbols_cache.json")

# ====== è‡ªå‹•å®‰è£å¿…è¦å¥—ä»¶ ======
def ensure_pkg(pkg: str):
    try:
        __import__(pkg)
    except ImportError:
        print(f"ğŸ”§ æ­£åœ¨å®‰è£ {pkg}...")
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", pkg])

def init_db():
    """è‡ªå‹•åˆå§‹åŒ–è³‡æ–™åº«çµæ§‹"""
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS stocks (
            date TEXT,
            symbol TEXT,
            open REAL,
            high REAL,
            low REAL,
            close REAL,
            volume INTEGER,
            PRIMARY KEY (date, symbol)
        )
    ''')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol ON stocks (symbol)')
    conn.commit()
    conn.close()
    print(f"ğŸ“ è³‡æ–™åº« {DB_NAME} å·²å°±ç·’")

def get_full_stock_list():
    """ç²å– A è‚¡æ¸…å–® (å››å±¤é˜²ç¦¦æ©Ÿåˆ¶)"""
    ensure_pkg("akshare")
    import akshare as ak
    
    threshold = 4000
    res = []

    # --- Level 1: æ¨™æº–æ¥å£ (stock_info_a_code_name) ---
    print("ğŸ“¡ [Level 1] å˜—è©¦ Akshare æ¨™æº–æ¥å£...")
    try:
        df = ak.stock_info_a_code_name()
        if not df.empty:
            df['code'] = df['code'].astype(str).str.zfill(6)
            valid_prefixes = ('000','001','002','300','600','601','603','605')
            df = df[df['code'].str.startswith(valid_prefixes)]
            res = [f"{c}.SS" if c.startswith('6') else f"{c}.SZ" for c in df['code']]
            if len(res) >= threshold:
                save_cache(res)
                print(f"âœ… Level 1 æˆåŠŸ ({len(res)} æª”)")
                return list(set(res))
    except Exception as e:
        print(f"âš ï¸ Level 1 ç•°å¸¸: {e}")

    # --- Level 2: å³æ™‚è¡Œæƒ…æ¥å£ (EM æ¥å£ï¼Œé€šå¸¸è¼ƒç©©å®š) ---
    print("ğŸ“¡ [Level 2] å˜—è©¦å³æ™‚è¡Œæƒ…æ¥å£ (EM)...")
    try:
        df_sh = ak.stock_sh_a_spot_em()
        df_sz = ak.stock_sz_a_spot_em()
        all_codes = []
        if not df_sh.empty: all_codes += df_sh['ä»£ç '].astype(str).str.zfill(6).tolist()
        if not df_sz.empty: all_codes += df_sz['ä»£ç '].astype(str).str.zfill(6).tolist()
        res = [f"{c}.SS" if c.startswith('6') else f"{c}.SZ" for c in all_codes]
        if len(res) >= threshold:
            save_cache(res)
            print(f"âœ… Level 2 æˆåŠŸ ({len(res)} æª”)")
            return list(set(res))
    except Exception as e:
        print(f"âš ï¸ Level 2 ç•°å¸¸: {e}")

    # --- Level 3: è®€å– Drive å¿«å– ---
    print(f"ğŸ“¡ [Level 3] å˜—è©¦è®€å–å¿«å–æª”...")
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r') as f:
                res = json.load(f)
            if len(res) >= threshold:
                print(f"â™»ï¸ Level 3 æˆåŠŸ: å¾å¿«å–æ¢å¾© {len(res)} æª”")
                return res
        except:
            pass

    # --- Level 4: æœ€çµ‚å‚™æ´ (æ ¸å¿ƒæ¬Šå€¼è‚¡) ---
    print("ğŸš¨ [Level 4] æ‰€æœ‰é€£ç·šå¤±æ•ˆä¸”ç„¡å¿«å–ï¼Œä½¿ç”¨æ¬Šå€¼è‚¡ä¿åº•...")
    return [
        "600519.SS", "601318.SS", "600036.SS", "601398.SS", "601857.SS", 
        "000858.SZ", "000333.SZ", "002415.SZ", "000001.SZ", "300750.SZ"
    ]

def save_cache(data):
    """å°‡æ¸…å–®å­˜å…¥å¿«å–"""
    try:
        os.makedirs(CACHE_DIR, exist_ok=True)
        with open(CACHE_FILE, 'w') as f:
            json.dump(data, f)
        print(f"ğŸ’¾ æ¸…å–®å·²å‚™ä»½è‡³: {CACHE_FILE}")
    except Exception as e:
        print(f"ğŸ“¦ å¿«å–å¯«å…¥å¤±æ•—: {e}")

def fetch_single_stock(symbol, period):
    """å–®æª”ä¸‹è¼‰é‚è¼¯"""
    try:
        time.sleep(random.uniform(0.6, 1.5)) # ç•¥å¾®æ‹‰é•·ç­‰å¾…æ™‚é–“ä¿è­·é€£ç·š
        tk = yf.Ticker(symbol)
        hist = tk.history(period=period, timeout=30)
        
        if hist is not None and not hist.empty:
            hist = hist.reset_index()
            hist.columns = [c.lower() for c in hist.columns]
            if 'date' in hist.columns:
                hist['date'] = pd.to_datetime(hist['date'], utc=True).dt.tz_localize(None).dt.strftime('%Y-%m-%d')
                hist['symbol'] = symbol
                return hist[['date', 'symbol', 'open', 'high', 'low', 'close', 'volume']]
    except:
        return None
    return None

def fetch_cn_market_data(is_first_time=False):
    """ä¸»é€²å…¥é»"""
    init_db()
    period = "max" if is_first_time else "7d"
    items = get_full_stock_list()
    
    print(f"ğŸš€ ä»»å‹™å•Ÿå‹•: {'å…¨é‡(max)' if is_first_time else 'å¢é‡(7d)'}, ç›®æ¨™: {len(items)} æª”")
    
    all_dfs = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(fetch_single_stock, tkr, period): tkr for tkr in items}
        count = 0
        for future in as_completed(futures):
            res = future.result()
            if res is not None:
                all_dfs.append(res)
            count += 1
            if count % 100 == 0:
                print(f"ğŸ“Š ä¸‹è¼‰é€²åº¦: {count}/{len(items)}...")

    if all_dfs:
        final_df = pd.concat(all_dfs, ignore_index=True)
        print(f"âœ¨ ä»»å‹™å®Œæˆï¼Œç²å– {len(final_df)} ç­†è¨˜éŒ„")
        return final_df
    return pd.DataFrame()

if __name__ == "__main__":
    df = fetch_cn_market_data(is_first_time=False)
