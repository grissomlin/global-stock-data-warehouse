# -*- coding: utf-8 -*-
import os, sys, sqlite3, json, time, gzip, shutil, socket, io
import pandas as pd
from datetime import datetime, timedelta
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload

# ğŸ’¡ å¢åŠ å…¨åŸŸé€£ç·šé€¾æ™‚ï¼Œç¢ºä¿å¤§æª”æ¡ˆå‚³è¼¸ç©©å®š (10åˆ†é˜)
socket.setdefaulttimeout(600)

# å°å…¥é€šçŸ¥èˆ‡ç’°å¢ƒè®Šæ•¸è¼‰å…¥å·¥å…·
from notifier import StockNotifier
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# åŒ¯å…¥å„åœ‹ä¸‹è¼‰æ¨¡çµ„
import downloader_tw, downloader_us, downloader_cn, downloader_hk, downloader_jp, downloader_kr

# ========== æ ¸å¿ƒåƒæ•¸è¨­å®š ==========
GDRIVE_FOLDER_ID = '1ltKCQ209k9MFuWV6FIxQ1coinV2fxSyl' 
SERVICE_ACCOUNT_FILE = 'citric-biplane-319514-75fead53b0f5.json'
AUDIT_DB_PATH = "data_warehouse_audit.db"

# ğŸ“Š æ•¸é‡é–€æª»é è­¦è¨­å®š
EXPECTED_MIN_ROWS = {
    'tw': 900, 'us': 4000, 'cn': 4500, 'hk': 1500, 'jp': 3000, 'kr': 2000
}

notifier = StockNotifier()

def get_drive_service():
    """åˆå§‹åŒ– Google Drive API æœå‹™"""
    env_json = os.environ.get('GDRIVE_SERVICE_ACCOUNT')
    try:
        if env_json:
            info = json.loads(env_json)
            creds = service_account.Credentials.from_service_account_info(info, scopes=['https://www.googleapis.com/auth/drive'])
        elif os.path.exists(SERVICE_ACCOUNT_FILE):
            creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])
        else:
            return None
        return build('drive', 'v3', credentials=creds, cache_discovery=False)
    except Exception as e:
        print(f"âŒ ç„¡æ³•åˆå§‹åŒ– Drive æœå‹™: {e}")
        return None

# ========== é›²ç«¯èˆ‡ç£ç¢Ÿç¶­è­·æ ¸å¿ƒé‚è¼¯ ==========

def download_backup_from_drive(service, file_name):
    """å¾é›²ç«¯ä¸‹è¼‰ .db.gz ä¸¦å„²å­˜è‡³æœ¬åœ°"""
    query = f"name = '{file_name}' and '{GDRIVE_FOLDER_ID}' in parents and trashed = false"
    results = service.files().list(q=query, fields="files(id)").execute(num_retries=3)
    items = results.get('files', [])
    
    if not items:
        return False

    file_id = items[0]['id']
    print(f"ğŸ“¡ ç™¼ç¾é›²ç«¯å‚™ä»½: {file_name}, æ­£åœ¨ä¸‹è¼‰...")
    request = service.files().get_media(fileId=file_id)
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request, chunksize=10*1024*1024)
    
    done = False
    while not done:
        status, done = downloader.next_chunk(num_retries=5)
        if status:
            print(f"ğŸ“¥ ä¸‹è¼‰é€²åº¦: {int(status.progress() * 100)}%")
    return True

def decompress_db(gz_file):
    """è§£å£“ç¸® .gz ç‚º .db ä¸¦ç«‹å³åˆªé™¤å£“ç¸®æª”é‡‹æ”¾ç©ºé–“"""
    db_file = gz_file.replace('.gz', '')
    try:
        print(f"ğŸ”“ æ­£åœ¨è§£å£“ {gz_file}...")
        with gzip.open(gz_file, 'rb') as f_in:
            with open(db_file, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        os.remove(gz_file) # ğŸ’¡ é‡‹æ”¾ GitHub Actions ç©ºé–“
        return True
    except Exception as e:
        print(f"âŒ è§£å£“å¤±æ•—: {e}")
        return False

def optimize_and_compress(db_file):
    """SQLite å„ªåŒ–èˆ‡ GZIP å£“ç¸®ï¼Œä¸¦æ¸…ç†åŸå§‹æª”"""
    gz_file = f"{db_file}.gz"
    try:
        print(f"ğŸ§¹ åŸ·è¡Œ VACUUM å„ªåŒ– {db_file}...")
        conn = sqlite3.connect(db_file)
        conn.execute("VACUUM")
        conn.close()
        
        print(f"ğŸ“¦ æ­£åœ¨å£“ç¸®ç‚º {gz_file}...")
        with open(db_file, 'rb') as f_in:
            with gzip.open(gz_file, 'wb', compresslevel=6) as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        os.remove(db_file) # ğŸ’¡ å£“ç¸®å®Œç•¢ç«‹å³åˆªé™¤æ•¸ GB çš„åŸå§‹æª”
        return gz_file
    except Exception as e:
        print(f"âŒ å£“ç¸®å¤±æ•—: {e}")
        return None

def upload_to_drive(service, file_path):
    """å°‡ .db.gz ä¸Šå‚³/æ›´æ–°è‡³ Google Drive"""
    file_name = os.path.basename(file_path)
    media = MediaFileUpload(file_path, mimetype='application/octet-stream', resumable=True, chunksize=10*1024*1024)
    
    query = f"name = '{file_name}' and '{GDRIVE_FOLDER_ID}' in parents and trashed = false"
    results = service.files().list(q=query, fields="files(id)").execute(num_retries=3)
    items = results.get('files', [])
    
    if items:
        request = service.files().update(fileId=items[0]['id'], media_body=media, supportsAllDrives=True)
    else:
        file_metadata = {'name': file_name, 'parents': [GDRIVE_FOLDER_ID]}
        request = service.files().create(body=file_metadata, media_body=media, supportsAllDrives=True)

    response = None
    while response is None:
        status, response = request.next_chunk(num_retries=5)
        if status:
            print(f"ğŸ“¤ ä¸Šå‚³é€²åº¦: {int(status.progress() * 100)}%")
    return True

# ========== ä¸»ç¨‹å¼åŸ·è¡Œå€å¡Š ==========

def main():
    target_market = sys.argv[1].lower() if len(sys.argv) > 1 else None
    module_map = {
        'tw': downloader_tw, 'us': downloader_us, 'cn': downloader_cn,
        'hk': downloader_hk, 'jp': downloader_jp, 'kr': downloader_kr
    }
    markets_to_run = [target_market] if target_market in module_map else module_map.keys()

    service = get_drive_service()
    if not service:
        print("âŒ éŒ¯èª¤ï¼šç„¡æ³•å•Ÿå‹• Drive æœå‹™ï¼Œè«‹æª¢æŸ¥èªè­‰é‡‘é‘°ã€‚")
        return

    for m in markets_to_run:
        db_file = f"{m}_stock_warehouse.db"
        gz_file = f"{db_file}.gz"
        try:
            print(f"\n--- ğŸŒ å¸‚å ´å•Ÿå‹•: {m.upper()} ---")

            # 1. æ¢å¾©å‚™ä»½é‚è¼¯
            if not os.path.exists(db_file):
                if download_backup_from_drive(service, gz_file):
                    decompress_db(gz_file)
                else:
                    print(f"ğŸ†• é›²ç«¯ç„¡å‚™ä»½ï¼Œå»ºç«‹å…¨æ–°è³‡æ–™åº«...")
                    conn = sqlite3.connect(db_file)
                    conn.execute('''CREATE TABLE IF NOT EXISTS stock_prices (
                        date TEXT, symbol TEXT, market TEXT, open REAL, high REAL, low REAL, close REAL, volume INTEGER, updated_at TEXT,
                        PRIMARY KEY (date, symbol, market))''')
                    conn.close()

            # 2. åŸ·è¡Œæ¨¡çµ„ä¸‹è¼‰
            target_module = module_map.get(m)
            stats = target_module.main() 
            
            # 3. é€šçŸ¥èˆ‡é›²ç«¯å°å­˜
            success_count = stats.get('success', 0)
            if success_count > 0:
                # å°è£ï¼šå„ªåŒ– + å£“ç¸® + åˆªé™¤åŸå§‹æª”
                final_gz = optimize_and_compress(db_file)
                if final_gz:
                    upload_to_drive(service, final_gz)
                    os.remove(final_gz) # æœ€çµ‚ç£ç¢Ÿæ¸…ç†
                
                # å¥åº·åº¦æª¢æŸ¥
                health_note = "âœ… æ•¸æ“šå®Œæ•´åº¦è‰¯å¥½ã€‚"
                if m in EXPECTED_MIN_ROWS and success_count < EXPECTED_MIN_ROWS[m]:
                    health_note = f"âš ï¸ <b>[è³‡æ–™ç•°å¸¸é è­¦]</b> æ›´æ–°æ•¸é‡ ({success_count}) ä½æ–¼é–€æª» ({EXPECTED_MIN_ROWS[m]})ï¼"
                
                # å”¯ä¸€å ±è¡¨ç™¼é€é»
                notifier.send_stock_report(m.upper(), None, pd.DataFrame(), health_note, stats)
                
            else:
                notifier.send_telegram(f"âŒ {m.upper()} ä»Šæ—¥ç„¡æ–°æ•¸æ“šæ›´æ–°ã€‚")
                if os.path.exists(db_file): os.remove(db_file)

        except Exception as e:
            err_detail = f"âŒ {m.upper()} ç³»çµ±å´©æ½°: {str(e)}"
            print(err_detail)
            notifier.send_telegram(err_detail)
            if os.path.exists(db_file): os.remove(db_file)
    
    print("\nâœ¨ æ‰€æœ‰å¸‚å ´ä»»å‹™å·²å®Œæˆã€‚")

if __name__ == "__main__":
    main()
