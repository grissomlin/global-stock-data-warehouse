# -*- coding: utf-8 -*-
import os, sys, sqlite3, json, time, gzip, shutil, socket
import pandas as pd
from datetime import datetime, timedelta
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
import io

# ğŸ’¡ å¢åŠ å…¨åŸŸé€£ç·šé€¾æ™‚ï¼Œç¢ºä¿å¤§æª”æ¡ˆå‚³è¼¸ç©©å®š
socket.setdefaulttimeout(600)

# å°å…¥é€šçŸ¥èˆ‡ç’°å¢ƒè®Šæ•¸è¼‰å…¥å·¥å…·
from notifier import StockNotifier
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# åŒ¯å…¥å„åœ‹ä¸‹è¼‰æ¨¡çµ„
import downloader_tw, downloader_us, downloader_cn, downloader_hk, downloader_jp, downloader_kr

# ========== æ ¸å¿ƒåƒæ•¸è¨­å®š ==========
GDRIVE_FOLDER_ID = '1ltKCQ209k9MFuWV6FIxQ1coinV2fxSyl' 
SERVICE_ACCOUNT_FILE = 'citric-biplane-319514-75fead53b0f5.json'
AUDIT_DB_PATH = "data_warehouse_audit.db"

# ğŸ“Š æ•¸é‡é–€æª»é è­¦
EXPECTED_MIN_ROWS = {
    'tw': 900, 'us': 4000, 'cn': 4500, 'hk': 1500, 'jp': 3000, 'kr': 2000
}

notifier = StockNotifier()

def get_drive_service():
    env_json = os.environ.get('GDRIVE_SERVICE_ACCOUNT')
    if env_json:
        info = json.loads(env_json)
        creds = service_account.Credentials.from_service_account_info(info, scopes=['https://www.googleapis.com/auth/drive'])
    elif os.path.exists(SERVICE_ACCOUNT_FILE):
        creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])
    else:
        return None
    return build('drive', 'v3', credentials=creds, cache_discovery=False)

# ========== é›²ç«¯èˆ‡ç£ç¢Ÿç¶­è­·å‡½å¼ ==========

def download_backup_from_drive(service, file_name):
    """å¾é›²ç«¯ä¸‹è¼‰ .db.gz æª”æ¡ˆ"""
    query = f"name = '{file_name}' and '{GDRIVE_FOLDER_ID}' in parents and trashed = false"
    results = service.files().list(q=query, fields="files(id)").execute(num_retries=3)
    items = results.get('files', [])
    
    if not items:
        return False

    file_id = items[0]['id']
    print(f"ğŸ“¡ ç™¼ç¾é›²ç«¯å‚™ä»½: {file_name}, æ­£åœ¨ä¸‹è¼‰...")
    request = service.files().get_media(fileId=file_id)
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request, chunksize=10*1024*1024)
    
    done = False
    while done is False:
        status, done = downloader.next_chunk(num_retries=5)
        if status:
            print(f"ğŸ“¥ ä¸‹è¼‰é€²åº¦: {int(status.progress() * 100)}%")
    return True

def decompress_db(gz_file):
    """è§£å£“ç¸®ä¸¦ç§»é™¤å£“ç¸®æª”ä»¥ç¯€çœç©ºé–“"""
    db_file = gz_file.replace('.gz', '')
    try:
        print(f"ğŸ”“ æ­£åœ¨è§£å£“ {gz_file}...")
        with gzip.open(gz_file, 'rb') as f_in:
            with open(db_file, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        os.remove(gz_file) # ğŸ’¡ é—œéµï¼šç«‹å³åˆªé™¤å£“ç¸®æª”é‡‹æ”¾ç©ºé–“
        return True
    except Exception as e:
        print(f"âŒ è§£å£“å¤±æ•—: {e}")
        return False

def optimize_and_compress(db_file):
    """å„ªåŒ– SQLite ä¸¦å£“ç¸®ï¼Œéš¨å¾Œç§»é™¤åŸå§‹æª”"""
    gz_file = f"{db_file}.gz"
    try:
        print(f"ğŸ§¹ åŸ·è¡Œ VACUUM å„ªåŒ– {db_file}...")
        conn = sqlite3.connect(db_file)
        conn.execute("VACUUM")
        conn.close()
        
        print(f"ğŸ“¦ æ­£åœ¨å£“ç¸®ç‚º {gz_file}...")
        with open(db_file, 'rb') as f_in:
            with gzip.open(gz_file, 'wb', compresslevel=6) as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        # ğŸ’¡ é—œéµï¼šå£“ç¸®æˆåŠŸå¾Œç«‹å³åˆªé™¤åŸå§‹ .db æª”æ¡ˆï¼Œç‚ºä¸Šå‚³æµç¨‹é¨°å‡ºç©ºé–“
        os.remove(db_file) 
        return gz_file
    except Exception as e:
        print(f"âŒ å£“ç¸®å¤±æ•—: {e}")
        return None

def upload_to_drive(service, file_path):
    """ä¸Šå‚³ .db.gz åˆ°é›²ç«¯"""
    file_name = os.path.basename(file_path)
    media = MediaFileUpload(file_path, mimetype='application/octet-stream', resumable=True, chunksize=10*1024*1024)
    
    query = f"name = '{file_name}' and '{GDRIVE_FOLDER_ID}' in parents and trashed = false"
    results = service.files().list(q=query, fields="files(id)").execute(num_retries=3)
    items = results.get('files', [])
    
    if items:
        request = service.files().update(fileId=items[0]['id'], media_body=media, supportsAllDrives=True)
    else:
        file_metadata = {'name': file_name, 'parents': [GDRIVE_FOLDER_ID]}
        request = service.files().create(body=file_metadata, media_body=media, supportsAllDrives=True)

    response = None
    while response is None:
        status, response = request.next_chunk(num_retries=5)
        if status:
            print(f"ğŸ“¤ ä¸Šå‚³é€²åº¦: {int(status.progress() * 100)}%")
    return True

# ========== ä¸»ç¨‹å¼é‚è¼¯ ==========

def main():
    target_market = sys.argv[1].lower() if len(sys.argv) > 1 else None
    module_map = {
        'tw': downloader_tw, 'us': downloader_us, 'cn': downloader_cn,
        'hk': downloader_hk, 'jp': downloader_jp, 'kr': downloader_kr
    }
    markets_to_run = [target_market] if target_market in module_map else module_map.keys()

    service = get_drive_service()
    if not service:
        print("âŒ ç„¡æ³•å•Ÿå‹• Google Drive æœå‹™")
        return

    for m in markets_to_run:
        try:
            db_file = f"{m}_stock_warehouse.db"
            gz_file = f"{db_file}.gz"
            print(f"\n--- ğŸŒ å¸‚å ´ä»»å‹™å•Ÿå‹•: {m.upper()} ---")

            # 1. å˜—è©¦æ¢å¾©å‚™ä»½
            if not os.path.exists(db_file):
                if download_backup_from_drive(service, gz_file):
                    decompress_db(gz_file)
                else:
                    print(f"ğŸ†• é›²ç«¯ç„¡å‚™ä»½ï¼Œå»ºç«‹å…¨æ–°è³‡æ–™åº«...")
                    conn = sqlite3.connect(db_file)
                    conn.execute('''CREATE TABLE IF NOT EXISTS stock_prices (
                        date TEXT, symbol TEXT, market TEXT, open REAL, high REAL, low REAL, close REAL, volume INTEGER, updated_at TEXT,
                        PRIMARY KEY (date, symbol, market))''')
                    conn.close()

            # 2. åŸ·è¡Œå¢é‡ä¸‹è¼‰
            target_module = module_map.get(m)
            stats = target_module.main() 
            
            # 3. è™•ç†å®Œæˆå¾Œçš„å°è£èˆ‡ä¸Šå‚³
            success_count = stats.get('success', 0)
            if success_count > 0:
                final_gz = optimize_and_compress(db_file)
                if final_gz:
                    upload_to_drive(service, final_gz)
                    os.remove(final_gz) # ğŸ’¡ æœ€çµ‚æ¸…ç†
                
                health_note = "âœ… æ•¸æ“šå®Œæ•´åº¦è‰¯å¥½ã€‚"
                if m in EXPECTED_MIN_ROWS and success_count < EXPECTED_MIN_ROWS[m]:
                    health_note = f"âš ï¸ <b>[è­¦å‘Š]</b> æ•¸é‡ ({success_count}) ä½æ–¼é–€æª»!"
                
                notifier.send_stock_report(m.upper(), None, pd.DataFrame(), health_note, stats)
            else:
                notifier.send_telegram(f"âŒ {m.upper()} ä»Šæ—¥ç„¡æ›´æ–°ã€‚")
                if os.path.exists(db_file): os.remove(db_file)

        except Exception as e:
            notifier.send_telegram(f"âŒ {m.upper()} å´©æ½°: {str(e)}")
            if os.path.exists(db_file): os.remove(db_file)
    
    print("\nâœ¨ ä»»å‹™çµæŸ")

if __name__ == "__main__":
    main()
